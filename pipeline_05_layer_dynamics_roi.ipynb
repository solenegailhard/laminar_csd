{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cbf05af-2f93-4b23-82df-884f4e5bf7fc",
   "metadata": {},
   "source": [
    "# Spatiotemporal layer dynamics underlying MERF in a specific ROI\n",
    "\n",
    "In this tutorial:\n",
    "- retreive source time series\n",
    "- define the region of interest (ROI)\n",
    "- compute the CSDs for these region\n",
    "- bin and extract layer-specific CSD activity\n",
    "- define the time window of interests (WOI)\n",
    "- average (using max or mean) the activity within these time\n",
    "- define the surface and interpolate this activity\n",
    "- plot the activity over time\n",
    "- find the clusters in each regions: assess their significativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a42c7-9477-415c-85e1-d0c2a9c22d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from joblib import Parallel, delayed\n",
    "import k3d\n",
    "from scipy import stats\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import glob\n",
    "from scipy.signal import resample\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "from lameg.laminar import compute_csd\n",
    "from lameg.simulate import run_current_density_simulation, run_dipole_simulation\n",
    "from lameg.invert import invert_ebb, coregister, load_source_time_series\n",
    "from lameg.util import get_fiducial_coords\n",
    "from lameg.viz import plot_csd\n",
    "import spm_standalone\n",
    "\n",
    "from utils import convert_native_to_fsaverage, get_bigbrain_layer_boundaries, get_roi_idx, find_clusters, compute_activity_over_time, extract_layer_csd\n",
    "\n",
    "from lameg.surf import interpolate_data\n",
    "from lameg.viz import show_surface, color_map\n",
    "import io\n",
    "from PIL import Image\n",
    "from base64 import b64decode\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18f0fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env OMPI_TMPDIR=/scratch\n",
    "%env TMPDIR=/scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d356a7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_id='sub-001'\n",
    "ses_id = 'ses-01'\n",
    "epoch='motor'\n",
    "c_idx=3\n",
    "subj_dir=os.path.join('/home/common/bonaiuto/cued_action_meg/derivatives/processed',subj_id)\n",
    "subj_dir_sss=os.path.join('/home/common/bonaiuto/cued_action_meg/derivatives/processed_sss',subj_id)\n",
    "subj_surf_dir=os.path.join(subj_dir,'surf')\n",
    "multilayer_mesh_fname = os.path.join(subj_surf_dir, 'multilayer.11.ds.link_vector.fixed.gii')\n",
    "pial_mesh_fname = os.path.join(subj_surf_dir,'pial.ds.link_vector.fixed.gii')\n",
    "\n",
    "# External dependencies of the doc\n",
    "mri_fname = os.path.join(subj_dir, 't1w.nii')\n",
    "smooth_file = os.path.join(subj_surf_dir, 'FWHM5.00_multilayer.11.ds.link_vector.fixed.mat')\n",
    "data_file=os.path.join(subj_dir_sss, ses_id, f'spm/pmcspm_converted_autoreject-{subj_id}-{ses_id}-{epoch}-epo.mat')\n",
    "out_dir=os.path.join('./data', subj_id, ses_id, f'{subj_id}_{ses_id}_c{c_idx}_{epoch}_model_inv')\n",
    "\n",
    "out_dir_chunks = os.path.join(out_dir, 'csd_chunks_signif')\n",
    "os.makedirs(out_dir_chunks, exist_ok=True)\n",
    "\n",
    "fiducial_fname='/home/common/bonaiuto/cued_action_meg/raw/participants.tsv'\n",
    "nas, lpa, rpa=get_fiducial_coords(subj_id, fiducial_fname)\n",
    "\n",
    "%env SUBJECTS_DIR=/home/common/bonaiuto/cued_action_meg/derivatives/processed/fs/\n",
    "pial_ds = 'pial.ds.gii'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253dbfad-a1ca-4afb-9cd7-ef5b3459f6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract base name and path of data file\n",
    "data_path, data_file_name = os.path.split(data_file)\n",
    "data_base = os.path.splitext(data_file_name)[0]\n",
    "\n",
    "# Copy data files to tmp directory\n",
    "shutil.copy(\n",
    "    os.path.join(data_path, f'{data_base}.mat'), \n",
    "    os.path.join(out_dir, f'{data_base}.mat')\n",
    ")\n",
    "shutil.copy(\n",
    "    os.path.join(data_path, f'{data_base}.dat'), \n",
    "    os.path.join(out_dir, f'{data_base}.dat')\n",
    ")\n",
    "shutil.copy(\n",
    "    mri_fname, \n",
    "    os.path.join(out_dir, 't1w.nii')\n",
    ")\n",
    "shutil.copy(\n",
    "    smooth_file, \n",
    "    os.path.join(out_dir, 'FWHM5.00_multilayer.11.ds.link_vector.fixed.mat')\n",
    ")\n",
    "shutil.copy(\n",
    "    multilayer_mesh_fname, \n",
    "    os.path.join(out_dir, 'multilayer.11.ds.link_vector.fixed.gii')\n",
    ")\n",
    "\n",
    "# Construct base file name for simulations\n",
    "mri_fname = os.path.join(out_dir, 't1w.nii')\n",
    "smooth_file = os.path.join(out_dir, 'FWHM5.00_multilayer.11.ds.link_vector.fixed.mat')\n",
    "multilayer_mesh_fname = os.path.join(out_dir, 'multilayer.11.ds.link_vector.fixed.gii')\n",
    "base_fname = os.path.join(out_dir, f'{data_base}.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27221528",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm = spm_standalone.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed74321-990b-4c4b-be23-f6fa4c052813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of vertices per layer\n",
    "mesh = nib.load(multilayer_mesh_fname)\n",
    "pial_mesh = nib.load(pial_mesh_fname)\n",
    "n_layers = 11\n",
    "verts_per_surf = int(mesh.darrays[0].data.shape[0]/n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00572316-c790-4b71-b68f-5ba08388e276",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_verts = [l*int(verts_per_surf) for l in range(n_layers)]\n",
    "layer_coords = mesh.darrays[0].data[layer_verts,:]\n",
    "thickness = np.sqrt(np.sum((layer_coords[0,:]-layer_coords[-1,:])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbb644c-a0bc-4cfc-8e97-54f20eb16439",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_rate = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ff4a5f-0ce9-437f-9beb-74fd5eded7bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Patch size to use for inversion (in this case it matches the simulated patch size)\n",
    "patch_size = 5\n",
    "# Number of temporal modes to use for EBB inversion\n",
    "n_temp_modes = 4\n",
    "\n",
    "# Coregister data to multilayer mesh\n",
    "coregister(\n",
    "    nas, \n",
    "    lpa, \n",
    "    rpa, \n",
    "    mri_fname, \n",
    "    multilayer_mesh_fname, \n",
    "    base_fname,\n",
    "    spm_instance=spm\n",
    ")\n",
    "\n",
    "# Run inversion - save MU for extraction of single trials\n",
    "[_,_,MU] = invert_ebb(\n",
    "    multilayer_mesh_fname, \n",
    "    base_fname, \n",
    "    n_layers, \n",
    "    patch_size=patch_size, \n",
    "    n_temp_modes=n_temp_modes,\n",
    "    return_mu_matrix=True,\n",
    "    spm_instance=spm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4013dcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MU_fname = os.path.join(out_dir, 'MU.npy') \n",
    "# np.save(MU_fname, MU)\n",
    "\n",
    "MU = np.load(MU_fname, allow_pickle = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195c58cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get source time series for each layer and vertex\n",
    "mean_layer_ts, time, _ = load_source_time_series(base_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b874558",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_layer_ts_fname = os.path.join(out_dir, 'layer_ts.npy') \n",
    "#np.save(mean_layer_ts_fname, mean_layer_ts)\n",
    "\n",
    "mean_layer_ts = np.load(mean_layer_ts_fname) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f15694-4202-4af9-9f74-20e5a5167ec9",
   "metadata": {},
   "source": [
    "##### Get the indexes of the vertices of the region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa47981e-6e7b-48b0-8a6b-fa49c9c88c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roi_idx = get_roi_idx(subj_id, subj_surf_dir, 'lh', ['precentral','paracentral','postcentral', \n",
    "#                                                      'rostralmiddlefrontal','caudalmiddlefrontal',\n",
    "#                                                      'superiorfrontal','parsopercularis',\n",
    "#                                                      'parstriangularis',\n",
    "#                                                      'caudalanteriorcingulate',\n",
    "#                                                     'frontalpole'], pial_mesh)\n",
    "\n",
    "roi_idx = get_roi_idx(subj_id, subj_surf_dir, 'lh', ['precentral','paracentral','postcentral'], pial_mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0899d48f-672c-463d-bccb-4ebda171baaa",
   "metadata": {},
   "source": [
    "Get the big brain layer boundaries in this region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f218614-353c-4e06-a996-b2e95a614ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bb_layer_bound = get_bigbrain_layer_boundaries(subj_id, subj_surf_dir, subj_coord=None)\n",
    "\n",
    "bb_layer_bound_fname = os.path.join(out_dir, 'bb_layer_bound.npy') \n",
    "#np.save(bb_layer_bound_fname, bb_layer_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154be21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_layer_bound = np.load(bb_layer_bound_fname, allow_pickle = True) \n",
    "bb_lb_roi = bb_layer_bound[:,roi_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada44857",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_lb_roi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a51325",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_lb_roi = bb_lb_roi.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf92b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract mean_layer_ts from these vertices\n",
    "mean_layer_ts_roi = mean_layer_ts[roi_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6363053b-2fd5-4827-bbd0-4f8298f817b7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Compute the CSD for only these vertices\n",
    "Here we compute in chunks and write them to disk to save temporary memory space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4c9dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_csd_for_vertex(vertex):\n",
    "    vert=[l*int(verts_per_surf)+vertex for l in range(n_layers)]\n",
    "    layer_coords = mesh.darrays[0].data[vert, :]\n",
    "    thickness = np.linalg.norm(layer_coords[0, :] - layer_coords[-1, :])\n",
    "    \n",
    "    csd, sm_csd = compute_csd(\n",
    "        mean_layer_ts[vert, :],\n",
    "        thickness, #as thickness is computed only for vertexes in roi_idx\n",
    "        sfreq = 600,\n",
    "        smoothing='cubic'\n",
    "    )\n",
    "    return csd, sm_csd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc92e83-bd35-46b5-a9ac-904843c1955c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saved_chunks = {\n",
    "    int(os.path.basename(f).split(\"_\")[1].split(\".\")[0])\n",
    "    for f in glob.glob(os.path.join(out_dir_chunks, \"csd_*.npy\"))\n",
    "}\n",
    "\n",
    "chunk_size = 500\n",
    "for i in range(0, len(roi_idx), chunk_size):\n",
    "    chunk_idx = i // chunk_size\n",
    "    if chunk_idx in saved_chunks:\n",
    "        print(f\"Skipping chunk {chunk_idx} (already saved)\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing chunk {chunk_idx}...\")\n",
    "\n",
    "    chunk = roi_idx[i:i + chunk_size]\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(compute_csd_for_vertex)(vertex) for vertex in chunk\n",
    "    )\n",
    "\n",
    "    results = [res for res in results if res[0] is not None]\n",
    "    if results:\n",
    "        csd_chunk, sm_csd_chunk = zip(*results)\n",
    "        #np.save(os.path.join(out_dir_chunks, f\"csd_{chunk_idx:04d}.npy\"), np.stack(csd_chunk))\n",
    "        np.save(os.path.join(out_dir_chunks, f\"smooth_csd_{chunk_idx:04d}.npy\"), np.stack(sm_csd_chunk))\n",
    "        print(f\"Saved chunk {chunk_idx} to disk\")\n",
    "    else:\n",
    "        print(f\"Chunk {chunk_idx} returned no valid results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7d957e-bae5-4f73-988c-ce917d9e5438",
   "metadata": {},
   "outputs": [],
   "source": [
    "#csd_files = sorted(glob.glob(f\"{out_dir_chunks}/csd_*.npy\"))\n",
    "smooth_files = sorted(glob.glob(f\"{out_dir_chunks}/smooth_csd_*.npy\"))\n",
    "\n",
    "#csd_emp_roi = np.concatenate([np.load(f) for f in csd_files])\n",
    "sm_csd_roi = np.concatenate([np.load(f) for f in smooth_files])\n",
    "\n",
    "#csd_emp_roi = np.stack(csd_emp_roi)\n",
    "sm_csd_roi = np.stack(sm_csd_emp_roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db814c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_csd_roi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33bf775-656a-4e34-bddc-2b51bfcfb4ed",
   "metadata": {},
   "source": [
    "### Extract layer-specific CSD\n",
    "Based on Big Brain Atlas layer boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add5d639-0ac3-4187-9e88-907d43c63703",
   "metadata": {},
   "outputs": [],
   "source": [
    "csd_L5 = extract_layer_csd(sm_csd_roi, bb_lb_roi, roi_idx, 'L5')\n",
    "csd_L2_3 = extract_layer_csd(sm_csd_roi, bb_lb_roi, roi_idx, 'L2_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e2e7be-067e-4fba-bfc2-109d7d6a3a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.linspace(-1, 1, 1201) #in case you don't have it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4030cd6-9072-475b-8058-abf515ccf741",
   "metadata": {},
   "source": [
    "### Extract in a specific time window\n",
    "\n",
    "You can either use specific time windows or create sliding time windows (with or without overlapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c238b0e-e749-480d-a9e3-9e5428378c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_size = 0.05  # 50 ms\n",
    "step_size = 0.05  # for non-overlapping windows; this reduce for overlap\n",
    "start_time = time[300] #start_time = time[0] \n",
    "end_time = time[900] #end_time = time[-1]\n",
    "\n",
    "#woi = [(-0.5, -0.1), (-0.1, 0), (0, 0.12), (0.12, 0.3)]\n",
    "baseline_woi_idx = [(0,300)] \n",
    "woi = []\n",
    "t = start_time\n",
    "while t + wd_size <= end_time:\n",
    "    woi.append((t, t + wd_size))\n",
    "    t += step_size\n",
    "\n",
    "woi_idx = [(\n",
    "    (np.abs(time - start)).argmin(),\n",
    "    (np.abs(time - end)).argmin()\n",
    ") for start, end in woi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c17b29-ac31-4196-9689-8b689626f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcsd_L5 = compute_activity_over_time(csd_L5, woi_idx, roi_idx, method='mean_abs')\n",
    "mcsd_L2_3 = compute_activity_over_time(csd_L2_3, woi_idx, roi_idx, method='mean_abs')\n",
    "#get the average activity\n",
    "m_pial_ts = compute_activity_over_time(mean_layer_ts_roi, woi_idx, roi_idx, method='mean_abs')\n",
    "\n",
    "#compute baseline activity\n",
    "baseline_mcsd_L5 = compute_activity_over_time(csd_L5, baseline_woi_idx, roi_idx, method='mean_abs')\n",
    "baseline_mcsd_L2_3 = compute_activity_over_time(csd_L2_3, baseline_woi_idx, roi_idx, method='mean_abs')\n",
    "baseline_pial_ts = compute_activity_over_time(mean_layer_ts_roi, baseline_woi_idx, roi_idx, method='mean_abs')\n",
    "\n",
    "#baseline correct the values: should probably do this before with the time series? \n",
    "mcsd_L5 = mcsd_L5 - baseline_mcsd_L5\n",
    "mcsd_L2_3 = mcsd_L2_3 - baseline_mcsd_L2_3\n",
    "m_pial_ts = m_pial_ts - baseline_pial_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d51544",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcsd_L5.shape # number of time windows x vertices mean activity in this time window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20954d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionnary with the absolute values\n",
    "group_data = {\n",
    "    'v_mcsd_L5': {'data': np.abs(mcsd_L5)},\n",
    "    'v_mcsd_L2_3': {'data': np.abs(mcsd_L2_3)},\n",
    "    'v_m_pial_ts': {'data': np.abs(m_pial_ts)},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeece99",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_data['v_m_pial_ts']['data'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41b72e9-8961-4be7-a087-592d8d9e2aa1",
   "metadata": {},
   "source": [
    "### Interpolate activity and plot it\n",
    "For this you need to define both the original and downsampled surface. You will pad all the non-roi vertices with 0 to then plot on the overall surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193a8b5d-aa62-4aa4-98df-63e911a3abcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_inflated=nib.load(os.path.join(subj_surf_dir, 'inflated.gii'))\n",
    "ds_inflated=nib.load(os.path.join(subj_surf_dir, 'inflated.ds.gii'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e36e202-8759-415e-af85-0faa57683a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_vertices_ds = ds_inflated.darrays[0].data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557ea417-6096-4276-b346-2e3845a893a5",
   "metadata": {},
   "source": [
    "(Optional) find the coordinates of the maximum activity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5292a989-674a-41b6-bb59-584b4b038ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "pial_coords = ds_inflated.darrays[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22c8ade-21fe-4e76-950e-ab110e4952db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in group_data:\n",
    "    coords_list = []\n",
    "    data_array = group_data[key]['data']\n",
    "    for t_wd_ix in range(len(woi_idx)):\n",
    "        max_ix = np.nanargmax(data_array[t_wd_ix])\n",
    "        coords_i = pial_coords[max_ix]\n",
    "        coords_list.append(coords_i)\n",
    "\n",
    "    group_data[key]['max_coords'] = np.array(coords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06803fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_data['v_m_pial_ts']['max_coords'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ce7430-28fc-4c85-a3fd-df66e4482394",
   "metadata": {},
   "source": [
    "Interpolate for display on the original inflated surface (can choose not to display on inflated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e54cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group_key in group_data:\n",
    "    data = group_data[group_key]['data']\n",
    "    # Pad the data with 0s when not in the selected vertices \n",
    "    data_to_inter = np.zeros((data.shape[0],nb_vertices_ds))\n",
    "    # fill in ROI absolute values for each time window\n",
    "    data_to_inter[:,roi_idx] = data\n",
    "    results = Parallel(n_jobs=-1, prefer=\"processes\")(\n",
    "        delayed(interpolate_data)(\n",
    "            orig_inflated,\n",
    "            ds_inflated,\n",
    "            data_to_inter[t_wd_ix,:]\n",
    "        )\n",
    "        for t_wd_ix in range(data.shape[0])\n",
    "    )\n",
    "    group_data[group_key]['interpolated'] = np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f6ad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_data['v_m_pial_ts']['interpolated'][2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593379fa-c42f-4ee5-b034-c9cc21c118f5",
   "metadata": {},
   "source": [
    "Plot a specific group and time window (could loop this later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f48e15f-8f23-43d5-a8b9-96e3252746d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_group = 'v_mcsd_L5'\n",
    "chosen_interp_data = group_data[chosen_group]['interpolated']\n",
    "chosen_t_ix = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4637172b-76c9-49a6-8621-214b1f4bc8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify the bounds: should be the same to compare between layers\n",
    "vmin, vmax = np.min(chosen_interp_data), np.max(chosen_interp_data)\n",
    "vmin, vmax = 0, 0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14ad913",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors,_ = color_map(\n",
    "        chosen_interp_data[chosen_t_ix], \n",
    "        \"Spectral_r\", \n",
    "        vmin, \n",
    "        vmax,\n",
    "        norm='N' #or 'TS' if centered around 0 (so not the absolute value)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f639d4-2fcf-46a0-97f4-50b93cf63f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #if want to do all: \n",
    "# colors_all = []\n",
    "# for t_wd_ix in range(len(woi_idx)):\n",
    "#         colors,_ = color_map(\n",
    "#         chosen_interp_data[t_wd_ix], ### PROBLEM HERE\n",
    "#         \"Spectral_r\", \n",
    "#         vmin, \n",
    "#         vmax,\n",
    "#         norm='N' #or 'TS' if centered around 0 (so not the absolute value)\n",
    "#         )\n",
    "#         colors_all.append(colors)\n",
    "# group_data[group_key]['color'] = np.array(colors_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a2d250-944b-4724-80c0-cf8281b650c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "precentral_view = [-178.9392420191627,-45.354421787933546,128.49784196033622,\n",
    "                   8, 2.5, -32,\n",
    "                   0.7760071820687688, -0.2981949773300084, 0.5641742717218174]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b08c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 3187\n",
    "coords_v = ds_inflated.darrays[0].data[v,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464f9321-9a23-42ee-86fe-3e5ed9df9806",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot = show_surface(orig_inflated, vertex_colors=colors, info=True, \n",
    "#                     camera_view=precentral_view, \n",
    "#                     coords = group_data[chosen_group]['coords'][chosen_t_ix], coord_size=1, coord_color=[0,0,255])\n",
    "\n",
    "plot = show_surface(orig_inflated, vertex_colors=colors, info=True, \n",
    "                    camera_view=precentral_view, \n",
    "                    coords = coords_v, coord_size=4, coord_color=[0,0,255])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5b5819-fa11-47c1-977e-8d0b5b0e7b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot.camera #to get the orientation we want and can replace precentral_view\n",
    "plot.fetch_screenshot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc47337-d86b-4df4-81a9-49b27f5e1b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_subplot_label(ax, label, x=-.21, y=1.225, fontsize=26):\n",
    "    ax.text(x, y, label,  # Adjust left of y-axis\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=fontsize, va='top', ha='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d012e3-2ee6-42e0-adbd-74ee1c916a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the screenshot\n",
    "image_data = b64decode(plot.screenshot)\n",
    "image = Image.open(io.BytesIO(image_data))\n",
    "image_array = np.array(image)\n",
    "\n",
    "# Set up figure and axis\n",
    "fig, ax = plt.subplots(figsize=(24, 16))\n",
    "ax.imshow(image_array)\n",
    "ax.axis('off')  # Hide axes\n",
    "\n",
    "# Create a ScalarMappable for the colorbar\n",
    "norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
    "scalar_mappable = cm.ScalarMappable(norm=norm, cmap=\"Spectral_r\")\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(scalar_mappable, ax=ax, shrink=0.5, aspect=20, pad=0.02)\n",
    "cbar.set_label(f\"{chosen_group} max power on tw{woi_idx[chosen_t_ix]}\", fontsize=20)\n",
    "\n",
    "# Set two ticks: at min (vmin) and max (vmax)\n",
    "cbar.set_ticks([vmin, vmax])\n",
    "cbar.set_ticklabels([f\"Low CSD:{round(vmin,2)}\", f\"High CSD:{round(vmax,2)}\"])\n",
    "\n",
    "add_subplot_label(ax, 'a', fontsize=54)\n",
    "\n",
    "# Show and save figure\n",
    "#plot_fname = os.path.join(out_dir,f'fig_{chosen_group}_csd_wd{chosen_t_ix}_roi_{method}'.png')\n",
    "#plt.savefig(plot_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc268a96-f889-4d30-a68b-c0a2d0b99158",
   "metadata": {},
   "source": [
    "### Find clusters\n",
    "Find clusters and assess their significance. One important point here is that to find clusters we need to find the faces in the downsampled inflated data - otherwise the clusters will not be found/ or accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941b6d81-88fb-43da-9501-3cb7ddfad8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = ds_inflated.darrays[1].data\n",
    "mask = np.all(np.isin(faces, roi_idx), axis=1)\n",
    "roi_faces_global = faces[mask]\n",
    "\n",
    "# Build mapping from global to local index\n",
    "global_to_local = {v: i for i, v in enumerate(roi_idx)}\n",
    "\n",
    "# Reindex faces from global to local indices\n",
    "roi_faces_local = np.array([[global_to_local[v] for v in tri] for tri in roi_faces_global])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5594ec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f011b333-deb5-4925-b87f-3baea1a26d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to assess significance of clusters: on mean using spatial permutations\n",
    "# !pip install scikit-learn\n",
    "# !pip install brainspace\n",
    "# from brainspace.null_models import SpinPermutations\n",
    "\n",
    "# n_permutations = 100\n",
    "# sphere_coords = ds_inflated.darrays[0].data[roi_idx,:]\n",
    "# spin_model = SpinPermutations(n_rep=n_permutations, random_state=42)\n",
    "# spin_model.fit(sphere_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438786aa-7a89-4b2c-9df7-3c42e5c61e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for group_name, group_info in group_data.items():\n",
    "#     data = group_info['data']  # (time windows x vertices)\n",
    "#     group_info['clusters'] = {}  # add cluster subdict\n",
    "\n",
    "#     for t_i, time_idx in enumerate(woi_idx):\n",
    "#         vals = np.abs(data[t_i][roi_idx])\n",
    "#         vals = np.nan_to_num(vals)\n",
    "\n",
    "#         cluster_thresh = np.nanpercentile(vals, thresh)\n",
    "#         mask = np.where(vals >= cluster_thresh)[0]\n",
    "\n",
    "#         clusters = find_clusters(roi_faces_local, mask, n_hops=3)\n",
    "#         obs_masses = [np.sum(vals[c]) for c in clusters]\n",
    "\n",
    "#         null_vals = spin_model.randomize(vals)\n",
    "\n",
    "#         null_masses = []\n",
    "#         for i in range(n_permutations):\n",
    "#             permuted_vals = null_vals[:, i]\n",
    "\n",
    "#             s_thresh = np.nanpercentile(permuted_vals, thresh)\n",
    "#             s_mask = np.where(permuted_vals >= s_thresh)[0]\n",
    "\n",
    "#             s_clusters = find_clusters(roi_faces_local, s_mask, n_hops=3)\n",
    "#             s_masses = [np.sum(permuted_vals[c]) for c in s_clusters] if s_clusters else [0]\n",
    "#             null_masses.append(np.max(s_masses))\n",
    "\n",
    "#         null_masses = np.array(null_masses)\n",
    "#         pvals = [np.mean(null_masses >= m) for m in obs_masses]\n",
    "\n",
    "#         sig_clusters = [c for c, p in zip(clusters, pvals) if p < 0.05]\n",
    "\n",
    "#         max_v_cluster = []\n",
    "#         for cluster in sig_clusters:\n",
    "#             cluster_vals = vals[cluster]\n",
    "#             max_c_idx = np.argmax(cluster_vals)\n",
    "#             max_v_idx = roi_idx[cluster[max_c_idx]]\n",
    "#             max_v_cluster.append(max_v_idx)\n",
    "\n",
    "#         # Store in group_data[group_name]['clusters']\n",
    "#         group_info['clusters'][t_i] = {\n",
    "#             \"sig_clusters\": sig_clusters,\n",
    "#             \"pvals\": pvals,\n",
    "#             \"max_vertex_ids\": max_v_cluster\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb05328",
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_vert_idx = []\n",
    "\n",
    "for group_name, group_info in group_data.items():\n",
    "    data = group_info['data']  # (time windows x roi_vertices)\n",
    "    group_info['clusters'] = {}  # add cluster subdict\n",
    "\n",
    "    for t_i, time_idx in enumerate(woi_idx):\n",
    "        vals = data[t_i]\n",
    "        vals = np.nan_to_num(vals)\n",
    "\n",
    "        cluster_thresh = np.nanpercentile(vals, thresh)\n",
    "        mask = np.where(vals >= cluster_thresh)[0]\n",
    "        \n",
    "        highest_vert_idx.append(mask)\n",
    "\n",
    "        clusters = find_clusters(roi_faces_local, mask, n_hops=3)\n",
    "        \n",
    "        max_v_cluster = []\n",
    "        for cluster in clusters:\n",
    "            cluster_vals = vals[cluster]\n",
    "            max_c_idx = np.argmax(cluster_vals)\n",
    "            max_v_idx = roi_idx[cluster[max_c_idx]]\n",
    "            max_v_cluster.append(max_v_idx)\n",
    "\n",
    "        # Store in group_data[group_name]['clusters']\n",
    "        group_info['clusters'][t_i] = {\n",
    "            \"clusters\": clusters,\n",
    "            \"max_vertex_ids\": max_v_cluster\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952fb37f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "group_data['v_mcsd_L2_3']['clusters'][chosen_t_ix]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c75e66",
   "metadata": {},
   "source": [
    "### Assess significance of clusters using single trials "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5a6cbe",
   "metadata": {},
   "source": [
    "For this we need to extract the source activity per trials, and it is too heavy at once on all roi_idx, so we do it per vertex, then compute the CSD, extract layer-specific activity and then write it to a dictionnary, this is done in a separate python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a2c794",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_fname_t=os.path.join(subj_dir_sss, ses_id, f'spm/pcspm_converted_autoreject-{subj_id}-{ses_id}-{epoch}-epo.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82d4848",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ts, time, _ = load_source_time_series(\n",
    "        base_fname_t,\n",
    "        mu_matrix=MU, #we base ourselves on the inversion matrix from averaged data\n",
    "        vertices=5195\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f724ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_filename_path = f'{out_dir_chunks}/group_data_st.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e32470-6c9d-4acb-a88b-461c40d0a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.path.getsize(h5_filename_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7771d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(h5_filename_path, 'r+') as h5f:\n",
    "    subset = h5f['v_mcsd_L5'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4d081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(n_bins):\n",
    "    plt.plot(woi_idx, subset[3, b, :, 0], label=f'bin {b}')\n",
    "    plt.xlabel('time(ms) - from -0.5 to 0.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92fea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = ds_inflated.darrays[1].data\n",
    "mask = np.all(np.isin(faces, unique_vertices[0:271]), axis=1)\n",
    "roi_faces_global = faces[mask]\n",
    "\n",
    "# Build mapping from global to local index\n",
    "global_to_local = {v: i for i, v in enumerate(unique_vertices[0:271])}\n",
    "\n",
    "# Reindex faces from global to local indices\n",
    "roi_faces_local = np.array([[global_to_local[v] for v in tri] for tri in roi_faces_global])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60de2df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "roi_faces_local.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa416b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "n_vertices = len(unique_vertices[0:271])\n",
    "adjacency = lil_matrix((n_vertices, n_vertices), dtype=int)\n",
    "\n",
    "for tri in roi_faces_local:\n",
    "    # tri is an array of 3 vertex indices [v0, v1, v2]\n",
    "    for i in range(3):\n",
    "        for j in range(i + 1, 3):\n",
    "            v1, v2 = tri[i], tri[j]\n",
    "            adjacency[v1, v2] = 1\n",
    "            adjacency[v2, v1] = 1  # symmetric adjacency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64e91d5",
   "metadata": {},
   "source": [
    "Procedure: compute single trials (here bins) mean power or layer-specific power, baseline corrected -> see where it deviates from 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae5f176",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stats = subset.transpose(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ee98d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "from mne.stats import spatio_temporal_cluster_1samp_test\n",
    "# on tailed (as we work here with asbolute values)\n",
    "tail = 1\n",
    "p_threshold = 0.05\n",
    "\n",
    "df = len(subset[0:271]) - 1\n",
    "t_thresh = scipy.stats.t.ppf(1 - p_threshold, df=df)\n",
    "\n",
    "n_permutations = 50\n",
    "\n",
    "# Run the analysis\n",
    "T_obs, clusters, cluster_p_values, H0 = clu = spatio_temporal_cluster_1samp_test(\n",
    "    data_stats,\n",
    "    adjacency=adjacency,\n",
    "    n_jobs=None,\n",
    "    threshold=t_thresh,\n",
    "    buffer_size=None,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c50936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the clusters that are statistically significant at p < 0.05\n",
    "good_clusters_idx = np.where(cluster_p_values < 0.05)[0]\n",
    "good_clusters = [clusters[idx] for idx in good_clusters_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b275241",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_clusters_idx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248cb4dc-c8ec-43da-b763-9a18f8022a46",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Plot the time series at the maximum peaks/clusters \n",
    "Can be clusters or a list/dictionnary of (n_vertices x n_time windows) for each layer/condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ee699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the woi you want to plot\n",
    "woi_index = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2672de31-f221-4146-b4ce-ec2692e68677",
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin_vmax = [-0.1, 0.1]\n",
    "\n",
    "for t_ix in woi_index:\n",
    "    for group_name, group_info in group_data.items():\n",
    "        max_vertex_ids = group_info['clusters'][t_ix][\"max_vertex_ids\"]\n",
    "        n_plots = len(max_vertex_ids)\n",
    "\n",
    "        if n_plots == 0:\n",
    "            continue\n",
    "\n",
    "        # Layout: 1 row per vertex, 2 columns per row (Time Series | CSD)\n",
    "        nrows = n_plots\n",
    "        ncols = 2\n",
    "        figsize = (12, 4 * nrows)\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "        if n_plots == 1:\n",
    "            axes = np.array([axes])  # Ensure axes is 2D even for 1 row\n",
    "\n",
    "        for i, max_ix in enumerate(max_vertex_ids):\n",
    "            # Time Series\n",
    "            ax_ts = axes[i, 0]\n",
    "            pial_layer_ts_mean = mean_layer_ts[max_ix, :]\n",
    "            start, end = woi[t_ix]\n",
    "            ax_ts.axvspan(start, end, color='gray', alpha=0.3)\n",
    "            ax_ts.plot(time, pial_layer_ts_mean, color='k')\n",
    "            ax_ts.set_title(f'{key} - Vertex {max_ix}\\nTime Series')\n",
    "            ax_ts.set_xlabel('Time (ms)')\n",
    "            ax_ts.set_ylabel('Amplitude')\n",
    "\n",
    "            # CSD\n",
    "            ax_csd = axes[i, 1]\n",
    "            roi_max_ix = np.where(roi_idx == max_ix)[0][0]\n",
    "            csd_data = sm_csd_roi[roi_max_ix]\n",
    "            plot_csd(csd_data, time, ax_csd, vmin_vmax=vmin_vmax, n_layers=n_layers)\n",
    "\n",
    "            for pos in bb_lb_roi[roi_max_ix]:\n",
    "                ax_csd.axhline(y=pos, color='b', linestyle='-.')\n",
    "            start, end = woi[t_ix]\n",
    "            ax_csd.axvspan(start, end, color='gray', alpha=0.3)\n",
    "            ax_csd.set_title(f'CSD - Vertex {max_ix}')\n",
    "            ax_csd.set_xlabel('Time (ms)')\n",
    "            ax_csd.set_ylabel('Layer')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa189d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
